{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Models\n",
    "(Author: Matthew Inkawhich)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to saving and loading models, there are three core functions to be familiar with:\n",
    "\n",
    "- torch.save: Saves a serialized object to disk. This function uses Python's `pickle` utility for serialization. Models,tensors,and dictionaries of kinds of objects can be saved using this function.\n",
    "- torch.load: Uses `pickle`'s unpickling facilities to deserialize pickled object files to memory. This function also facilitates the device to load the data into.\n",
    "- torch.nn.Module.load_state_dict: Loads a model's parameter dictionary using a deserialized state_dict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`state_dict`是python字典类型。在PyTorch中，网络训练的权重被保存在model的parameters中，state_dict以字典的形式将每一层的layer和他的权重保存起来。只有可学习的参数才会保存在state_dict中。不只是model,　optimizer也有自己的state_dict,其中保存了训练时的超参和优化器的状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "conv1.weight \t torch.Size([6, 3, 5, 5])\n",
      "conv1.bias \t torch.Size([6])\n",
      "conv2.weight \t torch.Size([16, 6, 5, 5])\n",
      "conv2.bias \t torch.Size([16])\n",
      "fc1.weight \t torch.Size([120, 400])\n",
      "fc1.bias \t torch.Size([120])\n",
      "fc2.weight \t torch.Size([84, 120])\n",
      "fc2.bias \t torch.Size([84])\n",
      "fc3.weight \t torch.Size([10, 84])\n",
      "fc3.bias \t torch.Size([10])\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'nesterov': False, 'dampening': 0, 'weight_decay': 0, 'lr': 0.001, 'params': [139829746445696, 139829746445984, 139829746445120, 139829746445840, 139829746445912, 139829746446056, 139829746446128, 139829746446200, 139829746446272, 139829746151496], 'momentum': 0.9}]\n"
     ]
    }
   ],
   "source": [
    "class TheModelClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TheModelClass, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,6,5)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        self.fc1 = nn.Linear(16*5*5,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1,16*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = TheModelClass()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.001,momentum=0.9)\n",
    "print('Model\\'s state_dict:')\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor,\"\\t\", model.state_dict()[param_tensor].size())\n",
    "    \n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存和载入模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),PATH)\n",
    "model = TheModelClass()\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在保存时，一般会使用.pt或者.pth作为文件扩展名"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存和载入整个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,PATH)\n",
    "model = torch.load(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The disadvantage of this approach is that the serialized data is bound to the specific classes and the exact directory structure used when the model is saved. The reason for this is because pickle does not save the model class itself. Rather, it saves a path to the file containing the class, which is used during load time. Because of this, your code can break in various ways when used in other projects or after refactors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存和载入checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'epoch':epoch,\n",
    "    'model_state_dict':model.state_dict(),\n",
    "    'optimizer_state_dict':optimizer.state_dict(),\n",
    "    'loss':loss,\n",
    "    ...\n",
    "},PATH)\n",
    "# 一般用.tar后缀的文件保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TheModelClass()\n",
    "optimizer = TheOptimizerClass()\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.eval()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存多个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'modelA_state_dict': modelA.state_dict(),\n",
    "    'modelB_state_dict': modelB.state_dict(),\n",
    "    'optimizerA_state_dict': optimizerA.state_dict(),\n",
    "    'optimizerB_state_dict': optimizerB.state_dict(),\n",
    "    ...\n",
    "},PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA = TheModelAClass()\n",
    "modelB = TheModelBCladd()\n",
    "optimizerA = TheOptimizerAClass()\n",
    "optimizerB = TheOptimizerBClass()\n",
    "checkpoint = torch.load(PATH)\n",
    "modelA.load_state_dict(checkpoint['modelA_state_dict'])\n",
    "modelB.load_state_dict(checkpoint['modelB_state_dict'])\n",
    "optimizerA.load_state_dict(checkpoint['optimizerA_state_dict'])\n",
    "optimizerB.load_state_dict(checkpoint['optimizerB_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warmstarting model using parameters from a deiifrent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(modelA.state_dict(), PATH)\n",
    "modelB = TheModelBClass()\n",
    "modelB.load_state_dict(torch.load(PATH),strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于某个模型，除了载入之前训练的参数外，还可以用其他模型的参数来赋值。`state_dict`本质上就是一个字典，只要关键字对的上，就能进行赋值。读取的模型参数可能会少一些层，也可能会多一些层，在加载时设置`strict = False`即可。另外，如果权重可以赋值，但关键字对不上，可以手动修改使其一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving&Loading Model Across Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save on GPU, load on CPU\n",
    "torch.save(model.state_dict(),PATH)\n",
    "device = torch.device('cpu')\n",
    "model = TheModelClass()\n",
    "model.load_state_dict(torch.load(PATH, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save on GPU, load on GPU\n",
    "torch.save(model.state_dict(),PATH)\n",
    "device = torch.device(\"cuda\")\n",
    "model = TheModelClass()\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save on CPU, load on GPU\n",
    "torch.save(model.state_dict(),PATH)\n",
    "device = torch.device(\"cuda\")\n",
    "model = TheModelClass()\n",
    "model.load_state_dict(torch.load(PATH,map_location=\"cuda\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving torch.nn.DataParallel Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.module.state_dict(),PATH)\n",
    "# load to whatever device you want"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
