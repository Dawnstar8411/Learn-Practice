{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn中包含两种接口：modules 和对应的function 版本。一般建议使用modules构建参数较多操作，例如layers; 使用function来构建参数较少的操作，例如激活函数，pooling 等。　PyTorch允许我们扩展torch.nn,torch.autograde,以及利用C库来自定义C扩展。若要构建一个**Linear**操作，需要完成两个部分：Module 和　Function。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, input_features, output_features,bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "        ## nn.Parameter is a special kind of Tensor, that will get automatically registered as Module's parameter\n",
    "        ## once it's assigned as an attribute. Parameters and buffers need to be registered, ot they won't be converted\n",
    "        ## when e.g. .cuda() is called. You can use .register_buffer() to register buffers.\n",
    "        ## nn.Parameters require gradients by default\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.Tensor(output_features, input_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_features))\n",
    "        else:\n",
    "            # You should always register all possible parameters, but the optional ones can be None if you want.\n",
    "            self.register_parameter('bias',None)\n",
    "        \n",
    "        self.weight.data.uniform_(-0.1,0.1)\n",
    "        if bias is not None:\n",
    "            self.bias.data.uniform_(-0.1,0.1)\n",
    "    \n",
    "    def forward(self,input):\n",
    "        # See the autograde section for explanation of what happens here.\n",
    "        return LinearFunction.apply(input, self.weight,self.bias)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        # (optional) Set the extra information about this module. You can test it by printing an object of this class\n",
    "        return 'in_features={}, out_features ={}, bias={}'.format(\n",
    "        self.in_features, self.out_features, self.bias is not None\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述对module的定义对parameter和buffer进行了注册，定义了前向传播函数和额外的自定义函数。如果要将新构建的操作添加到autograd中，是其可以自动求导，需要完成相应的**Function**内容。autograde通过function来实际计算结果和梯度，并记录操作历史。每一个新定义的function需要至少需要完成两个方法：forward()方法用来完成运算，backward()方法用来计算梯度."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearFunction(Function): # Inherit from Function\n",
    "    # note that both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    def forward(ctx, input,weight,bias=None):\n",
    "        ctx.save_for_backward(input,weight,bias)\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "    \n",
    "     # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output):\n",
    "        # This is a pattern that is very convenient-\n",
    "        # At the top of backward unpack saved_tensors and initialize all gradients\n",
    "        # Thank to the fact that additional trailing Nones are ignored, the return statement is simple\n",
    "        # even when the function has optional inputs.\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "        \n",
    "        # These needs_input_grad checks are optional and there only to improve efficiency\n",
    "        # if you want to make your code simpler, you can skip them.\n",
    "        # Returning gradients for inputs that don't requires it is not an error\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx_needs_input_grad[1]:\n",
    "            grad_weight = grad_ouput.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0).sequeeze(0)\n",
    "        \n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当需要使用自定义的function时，需要调用apply()方法：　linear = LinearFunction.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外给出一个没有可学习参数的function的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulConstant(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,tensor,constant):\n",
    "        # ctx is a context object that can be used to stash information for backward computation\n",
    "        ctx.constant = constant\n",
    "        return tensor * constant\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # We return as many input gradients as there were arguments.\n",
    "        # Gradients of Non-Tensor arguments to forward must be None\n",
    "        return grad_output * ctx.constant, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backward()函数的输入tensor也可以被追中历史，如果backward方法中包含有可微分操作，则会有更高阶的求导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过对比backward()求导和数值模拟的结果，可以检查所写的backward()是否正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import gradcheck\n",
    "# gradcheck takes a tuple of tensors as input, check if your gradient evaluated with these tensors are close\n",
    "# enough to numerical approaximations and returns True if they all verify this condition\n",
    "input = (torch.randn(20,20,dtype = troch.double,requires_grad =True),torch.randn(30,20,dtype=torch.double,requires_grad = True))\n",
    "test = gradcheck(linear,input,eps = 1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
