{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Runtime error(2): Out of Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Don't accumulate history across your training loop\n",
    "- Don't hold onto tensors and variables you don't need\n",
    "- Don't run RNNs on sequences that are too large\n",
    "- Don't use linear layers that are too large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "for i in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input)\n",
    "    loss = criterion(output)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss\n",
    "# 因为loss是一个需要求梯度的tensor，total_loss在整个循环的过程中会持续记录历史，占用显存，应该将loss里面的数据取出来\n",
    "# 如果是一个实数值，可以用.item；如果是tensor，可以用　.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    intermediate = f(input[i])\n",
    "    result += g(intermediate)\n",
    "output  = h(result)\n",
    "return output\n",
    "# 在　h 函数退出之后，intermediate依然存活且占用显存，因此需要将不用的中间变量及时删除\n",
    "# del intermediate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My GPU memory isn't freed properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch使用**caching memory allocator** 来加速显存分配，因此，`nvidia-smi`不能如实反映GPU显存使用情况。如果在PyTorch退出之后GPU显存依然没有释放，可能是因为某些Python的子进程还在运行，可以通过`ps -elf | grep python`找到这些进程病使用`kill -9 [pid]`杀掉进程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My recurrent network dosen't work with data parallelise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "未完待续。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "class MyModule(nn.Module):\n",
    "    # ... __init__, other methods\n",
    "    def forward(self,padder_input,input_lengths):\n",
    "        total_length = padded_input.size(1)\n",
    "        packed_input = pack_padded_sequence(padded_input, input_lengths,batch_first = True)\n",
    "        packed_output,_ = self.my_lstm(packed_input)\n",
    "        output,_=pad_packed_sequence(packed_output,batch_first = True, total_length = total_length)\n",
    "        return output\n",
    "\n",
    "m = MyModule().data()\n",
    "dp_m = nn.DataParallel(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
